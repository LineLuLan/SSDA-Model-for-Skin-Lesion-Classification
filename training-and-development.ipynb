{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":104884,"sourceType":"datasetVersion","datasetId":54339},{"sourceId":13875066,"sourceType":"datasetVersion","datasetId":8839749},{"sourceId":14044524,"sourceType":"datasetVersion","datasetId":8872118}],"dockerImageVersionId":31192,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# IT159IU: Self-Supervised Domain Adaptation for Skin Lesion Classification\n\n**Course:** Artificial Intelligence  \n**Team Members:**  \n- Trần Nam Anh (Leader) - ITDSIU23030  \n- Nguyễn Đức Hải (Member) - ITDSIU23006  \n\n---\n\n## Project Overview\nThis project addresses two critical challenges in medical AI: the high cost of data labeling and performance degradation due to \"Domain Shift.\" We implement and evaluate a **Self-Supervised Domain Adaptation (SSDA)** framework that combines **SimCLR** (Stage 1) and **DANN** (Stage 2) to leverage unlabeled medical data.\n\n## Research Pipeline\n1. **Data Engineering:** Stratified 60/40 \"Golden Split\" from HAM10000 dataset to simulate Source (labeled) and Target (unlabeled) domains.\n2. **Stage 1 (SSL):** Pre-training a ResNet-50 backbone using **SimCLR** (Contrastive Learning) on all 10,015 images to extract domain-specific features.\n3. **Stage 2 (DA):** Fine-tuning using **Domain-Adversarial Neural Networks (DANN)** with a **Gradient Reversal Layer (GRL)** to align features across domains.\n4. **Benchmarking:** Comparative analysis against a Supervised Baseline and an Ablation study (SSL-FT).\n\n## Key Technical Highlights\n- **Adversarial Training:** Uses GRL with a dynamic $\\lambda$ schedule to achieve domain-invariant representations.\n- **Handling Imbalance:** Implements Softened Alpha-Weighting ($\\alpha=0.3$) in Cross-Entropy Loss to stabilize training on imbalanced medical classes.\n- **Contrastive Logic:** Uses NT-Xent loss with numerical stability clamping to handle high-similarity medical textures.\n- ","metadata":{}},{"cell_type":"code","source":"import os\nimport shutil\nimport pandas as pd\n\n# ================================\n# 1. KHAI BÁO ĐƯỜNG DẪN TRONG KAGGLE\n# ================================\nkaggle_root = \"/kaggle/input/skin-cancer-mnist-ham10000\"\n\ncsv_file = os.path.join(kaggle_root, \"HAM10000_metadata.csv\")\nimage_part1 = os.path.join(kaggle_root, \"HAM10000_images_part_1\")\nimage_part2 = os.path.join(kaggle_root, \"HAM10000_images_part_2\")\n\n# Output folder (lưu lại trong Notebook)\noutput_root = \"/kaggle/working/HAM10000_converted\"\nos.makedirs(output_root, exist_ok=True)\n\nprint(\"CSV tồn tại:\", os.path.exists(csv_file))\nprint(\"Part1 tồn tại:\", os.path.exists(image_part1))\nprint(\"Part2 tồn tại:\", os.path.exists(image_part2))\n\n# ================================\n# 2. ĐỌC FILE METADATA\n# ================================\ndf = pd.read_csv(csv_file)\n\n# ================================\n# 3. GỘP ẢNH VÀ CHUYỂN THÀNH FOLDER-PER-CLASS\n# ================================\ncount = 0\n\nfor _, row in df.iterrows():\n    img_name = row[\"image_id\"] + \".jpg\"\n    label = str(row[\"dx\"])\n\n    # Tạo thư mục class tại working\n    target_dir = os.path.join(output_root, label)\n    os.makedirs(target_dir, exist_ok=True)\n\n    # Đường dẫn ảnh ở part1 và part2\n    img1 = os.path.join(image_part1, img_name)\n    img2 = os.path.join(image_part2, img_name)\n\n    # Copy ảnh sang thư mục mới\n    if os.path.exists(img1):\n        shutil.copy(img1, os.path.join(target_dir, img_name))\n        count += 1\n\n    elif os.path.exists(img2):\n        shutil.copy(img2, os.path.join(target_dir, img_name))\n        count += 1\n\n    else:\n        print(\"Không tìm thấy ảnh:\", img_name)\n\nprint(\"DONE! Tổng số ảnh copy:\", count)\nprint(\"Ảnh đã được lưu theo folder-per-class tại:\")\nprint(output_root)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 1. Data Loading and Preprocessing\n\nIn this section, we define the necessary data augmentations and create PyTorch DataLoaders. The dataset is split into three main parts for our experiments:\n\n- **`simclr_loader`**: Utilizes all 10,015 unlabeled images from the HAM10000 dataset for the self-supervised pre-training stage. It uses a `TwoCropTransform` to generate pairs of augmented views for contrastive learning.\n- **`src_loader`**: Contains the labeled source domain, a subset of HAM10000, used for supervised fine-tuning and training the classifier.\n- **`tgt_loader`**: Represents the target domain, used for evaluation. In the DANN experiment, the unlabeled images from this loader are also used to align the domain distributions.","metadata":{}},{"cell_type":"code","source":"import os\nimport glob\nimport random\nimport shutil\n\nrandom.seed(42)\n\n# ================================\n# ĐƯỜNG DẪN TRONG KAGGLE\n# ================================\ndrive_base = \"/kaggle/working/HAM10000_converted\"   # folder-per-class bạn vừa tạo\n\nsrc_root = \"/kaggle/working/HAM10000_split/source\"\ntgt_root = \"/kaggle/working/HAM10000_split/target\"\n\nos.makedirs(src_root, exist_ok=True)\nos.makedirs(tgt_root, exist_ok=True)\n\n# Lấy danh sách class\nclasses = os.listdir(drive_base)\nclasses = [c for c in classes if os.path.isdir(os.path.join(drive_base, c))]\n\nprint(\"Classes:\", classes)\n\n# ================================\n# SPLIT 60% SOURCE – 40% TARGET\n# ================================\nfor c in classes:\n    class_dir = os.path.join(drive_base, c)\n    imgs = glob.glob(os.path.join(class_dir, \"*.jpg\"))\n\n    if len(imgs) == 0:\n        print(\"Class rỗng:\", c)\n        continue\n\n    random.shuffle(imgs)\n    k = int(len(imgs) * 0.6)\n\n    src_imgs = imgs[:k]\n    tgt_imgs = imgs[k:]\n\n    # Tạo thư mục cho class\n    os.makedirs(os.path.join(src_root, c), exist_ok=True)\n    os.makedirs(os.path.join(tgt_root, c), exist_ok=True)\n\n    # Copy ảnh\n    for p in src_imgs:\n        shutil.copy(p, os.path.join(src_root, c, os.path.basename(p)))\n\n    for p in tgt_imgs:\n        shutil.copy(p, os.path.join(tgt_root, c, os.path.basename(p)))\n\n# ================================\n# THỐNG KÊ\n# ================================\nsrc_count = sum(len(os.listdir(os.path.join(src_root, c))) for c in os.listdir(src_root))\ntgt_count = sum(len(os.listdir(os.path.join(tgt_root, c))) for c in os.listdir(tgt_root))\n\nprint(\"DONE!\")\nprint(\"Source size:\", src_count)\nprint(\"Target size:\", tgt_count)\nprint(\"Output folder:\", \"/kaggle/working/HAM10000_split\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import shutil\nprint(\"Zipping the definitive data split...\")\nshutil.make_archive('golden_data_split', 'zip', '/kaggle/working/HAM10000_split')\nprint(\"Done. You can now download 'golden_data_split.zip' from the output.\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Technical Note: Medical-Specific Data Augmentation**\n\nFor skin lesion images, we applied a specific set of transforms:\n\n* **RandomVerticalFlip & RandomRotation:** Unlike natural images (where a tree is always upright), skin lesions have no intrinsic \"up\" or \"down\" orientation. Adding these increases geometric invariance.\n\n* **TwoCropTransform:** This is the core of SimCLR. It creates two \"views\" (noisy versions) of the same image to teach the model that they represent the same lesion despite different lighting or angles.","metadata":{}},{"cell_type":"markdown","source":"### Just run this cell if using the dataset \"it159-final-checkpoints\"\n","metadata":{}},{"cell_type":"code","source":"# ===================================================================\n# CELL: DATA LOADING FROM PRE-BUILT \"GOLDEN SPLIT\"\n# ===================================================================\nimport torch\nfrom torchvision import transforms\nfrom torch.utils.data import DataLoader\nfrom torchvision.datasets import ImageFolder\nimport os\n\n# --- 1. PATH DEFINITIONS ---\n# Path to the dataset containing our pre-built data splits and checkpoints\nGOLDEN_DATASET_ROOT = \"/kaggle/input/it159-final-checkpoints\" \n\n# Define the final, consistent paths for the DataLoaders\n# These paths point directly to the pre-split data within the input dataset.\nsrc_root = os.path.join(GOLDEN_DATASET_ROOT, \"golden_data_split/source\")\ntgt_root = os.path.join(GOLDEN_DATASET_ROOT, \"golden_data_split/target\")\n\n# The 'simclr_root' still needs all 10k images. We will need to create this.\n# Note: For the SimCLR pre-training stage, we still need to build the full\n# 'folder-per-class' structure from the raw HAM10000 data. The pre-split\n# data is only for the fine-tuning stage.\nsimclr_root_working = \"/kaggle/working/HAM10000_converted\"\nif not os.path.exists(simclr_root_working):\n    print(\"Creating full 'converted' directory for SimCLR pre-training...\")\n    # (This part is a simplified version of your original data prep)\n    import pandas as pd\n    import shutil\n    from tqdm.notebook import tqdm\n    \n    KAGGLE_ROOT = \"/kaggle/input/skin-cancer-mnist-ham10000\"\n    os.makedirs(simclr_root_working, exist_ok=True)\n    csv_file = os.path.join(KAGGLE_ROOT, \"HAM10000_metadata.csv\")\n    image_part1 = os.path.join(KAGGLE_ROOT, \"HAM10000_images_part_1\")\n    image_part2 = os.path.join(KAGGLE_ROOT, \"HAM10000_images_part_2\")\n    df = pd.read_csv(csv_file)\n    for _, row in tqdm(df.iterrows(), total=len(df), desc=\"Converting images for SimCLR\"):\n        img_name = row[\"image_id\"] + \".jpg\"\n        label = str(row[\"dx\"])\n        target_dir = os.path.join(simclr_root_working, label)\n        os.makedirs(target_dir, exist_ok=True)\n        img1 = os.path.join(image_part1, img_name)\n        img2 = os.path.join(image_part2, img_name)\n        if os.path.exists(img1): shutil.copy(img1, os.path.join(target_dir, img_name))\n        elif os.path.exists(img2): shutil.copy(img2, os.path.join(target_dir, img_name))\nelse:\n    print(\"Full 'converted' directory for SimCLR already exists.\")\n\n\n# --- 2. DATALOADER CREATION ---\nprint(\"\\n--- Creating DataLoaders from Golden Split ---\")\n\n# SimCLR augmentations\nsize = 224  # Nếu GPU yếu hoặc bị OOM, bạn có thể giảm xuống 128 lúc pre-train\n\ndef simclr_transform(size=size):\n    # Giảm hue xuống 0.05 để tránh biến màu da thành màu lạ\n    # Các thông số khác giữ 0.4 là ổn định\n    color_jitter = transforms.ColorJitter(brightness=0.4, contrast=0.4, saturation=0.4, hue=0.05)\n    \n    return transforms.Compose([\n        # Scale 0.5-1.0: Đảm bảo luôn lấy được phần lớn vùng bệnh\n        transforms.RandomResizedCrop(size=size, scale=(0.5, 1.0)),\n        \n        # --- BỔ SUNG QUAN TRỌNG CHO ẢNH Y TẾ ---\n        transforms.RandomHorizontalFlip(p=0.5),\n        transforms.RandomVerticalFlip(p=0.5),   # Bệnh da liễu không phân biệt trên dưới\n        transforms.RandomRotation(degrees=45),  # Xoay ảnh giúp model học tốt hơn\n        # ---------------------------------------\n\n        transforms.RandomApply([color_jitter], p=0.8),\n        transforms.RandomGrayscale(p=0.2),\n        \n        # GaussianBlur giúp model tập trung vào cấu trúc thay vì nhiễu hạt\n        transforms.GaussianBlur(kernel_size=3, sigma=(0.1, 2.0)),\n        \n        transforms.ToTensor(),\n        transforms.Normalize(mean=[0.485,0.456,0.406], std=[0.229,0.224,0.225])\n    ])\n\nclass TwoCropTransform:\n    \"\"\"Create two crops for contrastive learning\"\"\"\n    def __init__(self, base_transform):\n        self.base = base_transform\n        \n    def __call__(self, x):\n        q = self.base(x)\n        k = self.base(x)\n        return q, k\n\nbatch_simclr = 32\nbatch_ft = 32\n\n# SimCLR loader still uses the full dataset prepared in /kaggle/working\nsimclr_train_dataset = ImageFolder(root=simclr_root_working, transform=TwoCropTransform(simclr_transform(size)))\nsimclr_loader = DataLoader(simclr_train_dataset, batch_size=batch_simclr, shuffle=True, num_workers=2, drop_last=True)\n\n# FT/DANN loaders now use the direct paths from the input dataset\neval_transform = transforms.Compose([\n    transforms.Resize((size, size)),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n])\nsrc_dataset = ImageFolder(root=src_root, transform=eval_transform)\ntgt_dataset = ImageFolder(root=tgt_root, transform=eval_transform)\nsrc_loader = DataLoader(src_dataset, batch_size=batch_ft, shuffle=True, num_workers=2)\ntgt_loader = DataLoader(tgt_dataset, batch_size=batch_ft, shuffle=False, num_workers=2)\n\n# --- 3. FINAL INFORMATION ---\nprint(f\"\\nSimCLR dataset samples: {len(simclr_train_dataset)}\")\nprint(f\"Source labeled samples: {len(src_dataset)}\")\nprint(f\"Target samples: {len(tgt_dataset)}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-07T13:50:54.812539Z","iopub.execute_input":"2025-12-07T13:50:54.812731Z","iopub.status.idle":"2025-12-07T13:52:54.038585Z","shell.execute_reply.started":"2025-12-07T13:50:54.812712Z","shell.execute_reply":"2025-12-07T13:52:54.037653Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 2. Stage 1: Self-Supervised Pre-training (SimCLR)\n\nThis stage is the foundation of our advanced methods (SSDA and SSL-FT). We take a standard ResNet-50 model, pre-trained on ImageNet, and further fine-tune it using the SimCLR framework on our entire unlabeled HAM10000 dataset.\n\n**Objective:** To adapt the general visual features learned from ImageNet into domain-specific representations that are highly relevant to skin lesion characteristics.\n\nThe key components are:\n- **Encoder `f(·)`:** A ResNet-50 backbone that extracts feature vectors from input images.\n- **Projection Head `g(·)`:** A small MLP that maps the feature vectors into a latent space where contrastive loss is calculated.\n- **NT-Xent Loss:** The contrastive loss function that pulls augmented views of the same image (\"positive pairs\") together while pushing apart all other images (\"negative pairs\").","metadata":{}},{"cell_type":"code","source":"import torch, torchvision\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torchvision import transforms, models\nfrom torch.utils.data import DataLoader, Dataset\nimport numpy as np\nfrom tqdm import tqdm\nimport matplotlib.pyplot as plt\nfrom sklearn.manifold import TSNE\nfrom sklearn.metrics import accuracy_score, confusion_matrix, roc_auc_score\nimport seaborn as sns\nimport os\n\nDEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(\"Using device:\", DEVICE)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-07T13:53:20.663034Z","iopub.execute_input":"2025-12-07T13:53:20.663326Z","iopub.status.idle":"2025-12-07T13:53:21.905410Z","shell.execute_reply.started":"2025-12-07T13:53:20.663303Z","shell.execute_reply":"2025-12-07T13:53:21.904632Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# SimCLR augmentations\nsize = 224  # Nếu GPU yếu hoặc bị OOM, bạn có thể giảm xuống 128 lúc pre-train\n\ndef simclr_transform(size=size):\n    # Giảm hue xuống 0.05 để tránh biến màu da thành màu lạ\n    # Các thông số khác giữ 0.4 là ổn định\n    color_jitter = transforms.ColorJitter(brightness=0.4, contrast=0.4, saturation=0.4, hue=0.05)\n    \n    return transforms.Compose([\n        # Scale 0.5-1.0: Đảm bảo luôn lấy được phần lớn vùng bệnh\n        transforms.RandomResizedCrop(size=size, scale=(0.5, 1.0)),\n        \n        # --- BỔ SUNG QUAN TRỌNG CHO ẢNH Y TẾ ---\n        transforms.RandomHorizontalFlip(p=0.5),\n        transforms.RandomVerticalFlip(p=0.5),   # Bệnh da liễu không phân biệt trên dưới\n        transforms.RandomRotation(degrees=45),  # Xoay ảnh giúp model học tốt hơn\n        # ---------------------------------------\n\n        transforms.RandomApply([color_jitter], p=0.8),\n        transforms.RandomGrayscale(p=0.2),\n        \n        # GaussianBlur giúp model tập trung vào cấu trúc thay vì nhiễu hạt\n        transforms.GaussianBlur(kernel_size=3, sigma=(0.1, 2.0)),\n        \n        transforms.ToTensor(),\n        transforms.Normalize(mean=[0.485,0.456,0.406], std=[0.229,0.224,0.225])\n    ])\n\nclass TwoCropTransform:\n    \"\"\"Create two crops for contrastive learning\"\"\"\n    def __init__(self, base_transform):\n        self.base = base_transform\n        \n    def __call__(self, x):\n        q = self.base(x)\n        k = self.base(x)\n        return q, k\n\nprint(\"Đã khởi tạo SimCLR Transform tối ưu cho HAM10000 với size =\", size)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from torchvision.datasets import ImageFolder\nfrom torch.utils.data import DataLoader\nimport torchvision.transforms as transforms\n\n# # ================================\n# # ĐƯỜNG DẪN TRONG GOOGLE DRIVE\n# # ================================\n# simclr_root = \"/kaggle/working/HAM10000_converted\"\n# src_root = \"/kaggle/working/HAM10000_split/source\"\n# tgt_root = \"/kaggle/working/HAM10000_split/target\"\n\n# ================================\n# THAM SỐ\n# ================================\nbatch_simclr = 32\nbatch_ft = 32\n\n\n# ================================\n# SIMCLR DATASET (unlabeled)\n# ================================\nsimclr_train = ImageFolder(\n    root=simclr_root,\n    transform=TwoCropTransform(simclr_transform(size))\n)\n\nsimclr_loader = DataLoader(\n    simclr_train,\n    batch_size=batch_simclr,\n    shuffle=True,\n    num_workers=2,\n    drop_last=True\n)\n\n# ================================\n# FINE-TUNE DATASET (source + target)\n# ================================\neval_transform = transforms.Compose([\n    transforms.Resize((size, size)),\n    transforms.ToTensor(),\n    transforms.Normalize(\n        mean=[0.485, 0.456, 0.406],\n        std=[0.229, 0.224, 0.225]\n    )\n])\n\nsrc_dataset = ImageFolder(root=src_root, transform=eval_transform)\ntgt_dataset = ImageFolder(root=tgt_root, transform=eval_transform)\n\nsrc_loader = DataLoader(src_dataset, batch_size=batch_ft, shuffle=False, num_workers=2)\ntgt_loader = DataLoader(tgt_dataset, batch_size=batch_ft, shuffle=False, num_workers=2)\n\n# ================================\n# THÔNG TIN\n# ================================\nprint(\"SimCLR dataset samples:\", len(simclr_train))\nprint(\"Source labeled samples:\", len(src_dataset))\nprint(\"Target samples:\", len(tgt_dataset))\nprint(\"Classes:\", src_dataset.classes)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Run this cell for load backbone Resnet50 ","metadata":{}},{"cell_type":"code","source":"# ==========================================\n# CELL 1: BACKBONE RESNET50 & PROJECTION HEAD\n# ==========================================\n\n# 1. Load ResNet50 Pretrained\nbackbone = models.resnet50(weights=models.ResNet50_Weights.DEFAULT)\nfeat_dim = backbone.fc.in_features  # ResNet50 = 2048 features\n\n# 2. Tạo Encoder (bỏ lớp FC cuối)\nmodules = list(backbone.children())[:-1] \nencoder = nn.Sequential(*modules).to(DEVICE)\n\n# 3. Projection Head (Cập nhật theo feat_dim mới)\nclass ProjectionHead(nn.Module):\n    def __init__(self, in_dim, proj_dim=128):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Linear(in_dim, in_dim),\n            nn.ReLU(),\n            nn.Linear(in_dim, proj_dim)\n        )\n    def forward(self, x):\n        return self.net(x)\n\nproj = ProjectionHead(feat_dim, proj_dim=128).to(DEVICE)\n\nprint(f\"Encoder ResNet50 ready. Feature Dim: {feat_dim}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-07T13:53:28.616635Z","iopub.execute_input":"2025-12-07T13:53:28.617062Z","iopub.status.idle":"2025-12-07T13:53:29.849227Z","shell.execute_reply.started":"2025-12-07T13:53:28.617040Z","shell.execute_reply":"2025-12-07T13:53:29.848467Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"lr = 3e-4\nopt = torch.optim.Adam(list(encoder.parameters()) + list(proj.parameters()), lr=lr)\nprint(\"Optimizer ready. lr=\", lr)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def nt_xent_loss(z_i, z_j, temp=0.8, eps=1e-8):\n    z = torch.cat([z_i, z_j], dim=0)  # 2B x D\n    z = F.normalize(z, dim=1)\n    sim = torch.matmul(z, z.T)  # 2B x 2B\n    batch_size = z_i.shape[0]\n    mask = (~torch.eye(2*batch_size, dtype=torch.bool)).to(DEVICE)\n    sim = sim / temp\n    # clamp to avoid extreme values\n    sim = torch.clamp(sim, min=-50.0, max=50.0)\n    exp_sim = torch.exp(sim) * mask\n    denom = exp_sim.sum(dim=1) + eps\n    pos = torch.exp( (F.cosine_similarity(z_i, z_j, dim=-1)) / temp )\n    pos = torch.cat([pos, pos], dim=0)\n    loss = -torch.log(pos / denom + eps)\n    return loss.mean()\n\nprint(\"nt_xent_loss defined (temp=0.8 by default).\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"encoder.train(); proj.train()\nbatch = next(iter(simclr_loader))\n(x1x2, _) = batch\nx1, x2 = x1x2[0].to(DEVICE), x1x2[1].to(DEVICE)  # phù hợp với TwoCropTransform\n\nwith torch.set_grad_enabled(True):\n    h1 = encoder(x1).squeeze()\n    h2 = encoder(x2).squeeze()\n    z1 = proj(h1)\n    z2 = proj(h2)\n    loss = nt_xent_loss(z1, z2, temp=0.8)\n    print(\"Single-batch loss:\", loss.item())\n    assert not (torch.isnan(loss) or torch.isinf(loss)), \"Loss is NaN or Inf — stop!\"\nprint(\"Single-batch forward/backward check OK (loss finite).\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ==========================================\n# CELL 2: SIMCLR TRAINING (TĂNG EPOCH)\n# ==========================================\nimport os\nfrom tqdm import tqdm\n\n# Tăng epoch lên 50 để ResNet50 đủ thời gian học features\nsimclr_epochs = 50 \nckpt_dir = \"/kaggle/working/HAM10000_outputs/simclr\"\nos.makedirs(ckpt_dir, exist_ok=True)\n\n# Re-init optimizer cho SimCLR (chỉ train encoder & proj)\n# Ở giai đoạn này chưa cần chỉnh LR kỹ, để mặc định 3e-4 hoặc 1e-4 đều ổn\nopt = torch.optim.Adam(list(encoder.parameters()) + list(proj.parameters()), lr=3e-4)\n\nlosses = []\n\nprint(f\"Bắt đầu train SimCLR trong {simclr_epochs} epochs...\")\n\nfor epoch in range(simclr_epochs):\n    encoder.train()\n    proj.train()\n    running = 0.0\n\n    pbar = tqdm(simclr_loader,\n                desc=f\"SimCLR Epoch {epoch+1}/{simclr_epochs}\",\n                dynamic_ncols=True,\n                leave=False)\n\n    for (x1x2, _) in pbar:\n        x1, x2 = x1x2[0].to(DEVICE), x1x2[1].to(DEVICE)\n\n        # Forward\n        h1 = encoder(x1).squeeze() # [B, 2048]\n        h2 = encoder(x2).squeeze()\n        z1 = proj(h1)\n        z2 = proj(h2)\n\n        loss = nt_xent_loss(z1, z2, temp=0.8)\n\n        if torch.isnan(loss) or torch.isinf(loss):\n            print(\"Loss bị NaN/Inf. Dừng lại.\")\n            break\n\n        opt.zero_grad()\n        loss.backward()\n        opt.step()\n\n        running += loss.item()\n        pbar.set_postfix({\"loss\": running / (pbar.n + 1)})\n\n    epoch_loss = running / len(simclr_loader)\n    losses.append(epoch_loss)\n\n    # Lưu checkpoint mỗi 10 epoch hoặc epoch cuối\n    if (epoch + 1) % 10 == 0 or (epoch + 1) == simclr_epochs:\n        torch.save(\n            {\"encoder\": encoder.state_dict(),\n             \"proj\": proj.state_dict(),\n             \"epoch\": epoch+1},\n            os.path.join(ckpt_dir, f\"simclr_epoch_{epoch+1}.pth\")\n        )\n\nprint(\"SimCLR Pre-train hoàn tất!\")\nplt.plot(losses)\nplt.title(\"SimCLR Loss\")\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 3. Stage 2: Comparative Fine-tuning & Evaluation\n\nAfter obtaining a powerful, domain-specific encoder from the SimCLR pre-training stage, we proceed to Stage 2. Here, we conduct a series of comparative experiments to determine the most effective strategy for the final classification task.\n\nWe will evaluate three distinct approaches:\n- **A. SSDA (Our Proposed Method):** Combines the SimCLR encoder with DANN for explicit domain alignment.\n- **B. SSL-FT (Ablation Study):** Uses the SimCLR encoder but with a simpler supervised fine-tuning, to isolate the effect of SSL.\n- **C. Supervised Baseline:** A strong, standard approach using an ImageNet pre-trained encoder without any SimCLR fine-tuning.\n\n**Note:** For a fair comparison, all fine-tuning experiments (A, B, and C) will use the same optimized training configuration, including the SGD optimizer, a StepLR scheduler, and weighted cross-entropy loss to handle class imbalance.# ","metadata":{}},{"cell_type":"code","source":"# ckpt = torch.load(\"/kaggle/working/HAM10000_outputs/simclr/simclr_epoch_50.pth\", map_location=DEVICE) <-- USING IT WHEN AFTER TRAIN AND HAVING IN output\nckpt = torch.load(\"/kaggle/input/it159-final-checkpoints/simclr_outputs/simclr_epoch_50(x1)(no_freeze).pth\", map_location=DEVICE)\nencoder.load_state_dict(ckpt[\"encoder\"])\nproj.load_state_dict(ckpt[\"proj\"])\nprint(\"SIMCLR weights loaded!\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-07T12:31:52.242108Z","iopub.execute_input":"2025-12-07T12:31:52.242406Z","iopub.status.idle":"2025-12-07T12:31:54.774550Z","shell.execute_reply.started":"2025-12-07T12:31:52.242387Z","shell.execute_reply":"2025-12-07T12:31:54.773856Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Experiment A: SSDA (Self-Supervised Domain Adaptation)\n\nThis is our main proposed framework. It leverages the SimCLR-pretrained encoder as a starting point. During fine-tuning, it is trained on the labeled source data for classification while simultaneously using the DANN architecture to align the feature distributions of the unlabeled target data.\n\n- **Hypothesis:** By explicitly minimizing the domain shift, this method should achieve the best generalization performance on the target domain.","metadata":{}},{"cell_type":"code","source":"# =======================================================\n# CELL 3: SETUP DANN \n# =======================================================\nfrom collections import Counter\nimport numpy as np\nimport torch\nimport torch.nn as nn\n\n# 1. Định nghĩa các Head\n# class Classifier(nn.Module):\n#     def __init__(self, feat_dim, num_classes):\n#         super().__init__()\n#         self.fc = nn.Linear(feat_dim, num_classes)\n#     def forward(self, x): return self.fc(x)\n\nclass Classifier(nn.Module):\n    def __init__(self, feat_dim, num_classes):\n        super().__init__()\n        # Thêm một lớp ẩn với ReLU và Dropout\n        self.net = nn.Sequential(\n            nn.Linear(feat_dim, 512),\n            nn.ReLU(),\n            nn.Dropout(0.5),\n            nn.Linear(512, num_classes)\n        )\n    def forward(self, x): \n        return self.net(x)\n\nclass DomainDiscriminator(nn.Module):\n    def __init__(self, feat_dim):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Linear(feat_dim, 256), \n            nn.ReLU(), \n            nn.Dropout(0.5),\n            nn.Linear(256, 2)\n        )\n    def forward(self, x): return self.net(x)\n\n# Khởi tạo các Head\nclasses_list = src_dataset.classes # <-- Biến đúng là 'classes_list'\nclassifier = Classifier(feat_dim, len(classes_list)).to(DEVICE)\ndomain_disc = DomainDiscriminator(feat_dim).to(DEVICE)\n\n# 2. Tính toán Class Weights (Phiên bản \"mềm\")\nprint(\"Đang tính toán class weights (phiên bản 'mềm' hơn)...\")\nall_labels = src_loader.dataset.targets\nclass_counts = Counter(all_labels)\nclass_counts = [class_counts[i] for i in sorted(class_counts)]\n\nalpha = 0.3\nweights = 1.0 / (torch.tensor(class_counts, dtype=torch.float) ** alpha)\n# SỬA LỖI Ở ĐÂY: Dùng 'classes_list' thay vì 'classes'\nclass_weights = (weights / weights.sum() * len(classes_list)).to(DEVICE)\nprint(f\"Class Weights đã được tinh chỉnh (alpha={alpha}): {class_weights}\")\n\n# 3. Định nghĩa Optimizer (SGD) và Scheduler\nlr_enc = 1e-4\nlr_cls = 1e-3\n\nopt = torch.optim.SGD([\n    {'params': encoder.parameters(), 'lr': lr_enc},\n    {'params': classifier.parameters(), 'lr': lr_cls},\n    {'params': domain_disc.parameters(), 'lr': lr_cls}\n], momentum=0.9, weight_decay=5e-4)\n\nscheduler = torch.optim.lr_scheduler.StepLR(opt, step_size=10, gamma=0.1)\n\n# 4. Định nghĩa các hàm Loss\ncriterion_cls = nn.CrossEntropyLoss(weight=class_weights)\ncriterion_dom = nn.CrossEntropyLoss()\n\nprint(\"DANN Setup Done. Using softened weights, SGD optimizer, and StepLR scheduler.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-07T13:53:49.296223Z","iopub.execute_input":"2025-12-07T13:53:49.297103Z","iopub.status.idle":"2025-12-07T13:53:49.582046Z","shell.execute_reply.started":"2025-12-07T13:53:49.297073Z","shell.execute_reply":"2025-12-07T13:53:49.581285Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from torch.autograd import Function\n\nclass GradReverse(Function):\n    @staticmethod\n    def forward(ctx, x, lambd=1.0):\n        ctx.lambd = lambd\n        return x.view_as(x)\n\n    @staticmethod\n    def backward(ctx, grad_output):\n        return grad_output.neg() * ctx.lambd, None\n\ndef grad_reverse(x, lambd=1.0):\n    return GradReverse.apply(x, lambd)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-07T13:53:53.802544Z","iopub.execute_input":"2025-12-07T13:53:53.802867Z","iopub.status.idle":"2025-12-07T13:53:53.807907Z","shell.execute_reply.started":"2025-12-07T13:53:53.802813Z","shell.execute_reply":"2025-12-07T13:53:53.807076Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ==========================================\n# CELL 4: DANN TRAINING (DYNAMIC LAMBDA)\n# ==========================================\ndann_epochs = 20 # Tăng nhẹ epoch fine-tune nếu cần\nos.makedirs(\"/kaggle/working/ssl_dann_outputs\", exist_ok=True)\n\n# Hàm Gradient Reverse Layer\nclass GradReverse(torch.autograd.Function):\n    @staticmethod\n    def forward(ctx, x, lambd=1.0):\n        ctx.lambd = lambd\n        return x.view_as(x)\n    @staticmethod\n    def backward(ctx, grad_output):\n        return grad_output.neg() * ctx.lambd, None\n\ndef grad_reverse(x, lambd=1.0):\n    return GradReverse.apply(x, lambd)\n\ntrain_losses = []\ndomain_losses = []\naccs = []\n\nprint(f\"Bắt đầu Fine-tune DANN trong {dann_epochs} epochs...\")\n\nfor epoch in range(dann_epochs):\n    encoder.train()\n    classifier.train()\n    domain_disc.train()\n\n    running_cls = 0.0\n    running_dom = 0.0\n    total = 0\n    correct = 0\n    \n    # Lấy min length để zip không bị lỗi\n    len_dataloader = min(len(src_loader), len(tgt_loader))\n    \n    # Dùng zip để lấy cặp dữ liệu Source - Target\n    pbar = tqdm(enumerate(zip(src_loader, tgt_loader)),\n                total=len_dataloader,\n                desc=f\"DANN Epoch {epoch+1}/{dann_epochs}\")\n\n    for batch_idx, ((xs, ys), (xt, _)) in pbar:\n        xs, ys, xt = xs.to(DEVICE), ys.to(DEVICE), xt.to(DEVICE)\n\n        # -------------------------------------------\n        # 1. TÍNH DYNAMIC LAMBDA (Quan trọng)\n        # -------------------------------------------\n        # p chạy từ 0 đến 1 trong suốt quá trình training\n        p = float(batch_idx + epoch * len_dataloader) / (dann_epochs * len_dataloader)\n        # Lambda tăng dần theo hàm sigmoid: 0 -> 1\n        lambda_grl = 2. / (1. + np.exp(-10 * p)) - 1\n        \n        # -------------------------------------------\n        # 2. Forward Pass\n        # -------------------------------------------\n        # Feature extraction\n        feat_s = encoder(xs).squeeze() # [B, 2048]\n        feat_t = encoder(xt).squeeze()\n\n        # Classification Loss (Chỉ trên Source)\n        pred_s = classifier(feat_s)\n        loss_cls = criterion_cls(pred_s, ys)\n\n        # Domain Loss (Trên cả Source và Target)\n        feat_all = torch.cat([feat_s, feat_t], dim=0)\n        domain_labels = torch.cat([\n            torch.zeros(feat_s.size(0)), # Source = 0\n            torch.ones(feat_t.size(0))   # Target = 1\n        ]).long().to(DEVICE)\n\n        # Áp dụng Gradient Reversal với Dynamic Lambda\n        feat_rev = grad_reverse(feat_all, lambd=lambda_grl)\n        dom_pred = domain_disc(feat_rev)\n        loss_dom = criterion_dom(dom_pred, domain_labels)\n\n        # Tổng Loss\n        loss = loss_cls + loss_dom\n\n        # -------------------------------------------\n        # 3. Backward & Update\n        # -------------------------------------------\n        opt.zero_grad()\n        loss.backward()\n        opt.step()\n\n        # -------------------------------------------\n        # 4. Metrics & Logging\n        # -------------------------------------------\n        running_cls += loss_cls.item()\n        running_dom += loss_dom.item()\n\n        preds = pred_s.argmax(dim=1)\n        total += ys.size(0)\n        correct += (preds == ys).sum().item()\n\n        pbar.set_postfix({\n            'L_cls': f\"{running_cls/(batch_idx+1):.3f}\",\n            'L_dom': f\"{running_dom/(batch_idx+1):.3f}\",\n            'Acc': f\"{correct/total:.3f}\",\n            'λ': f\"{lambda_grl:.2f}\"  # In ra lambda để kiểm tra\n        })\n\n    scheduler.step()\n    \n    # Lưu metrics cuối epoch\n    train_losses.append(running_cls/len_dataloader)\n    accs.append(correct/total)\n    \n    \n    # Lưu checkpoint\n    torch.save({\n        'encoder': encoder.state_dict(),\n        'classifier': classifier.state_dict(),\n        'domain_disc': domain_disc.state_dict(),\n        'epoch': epoch\n    }, f\"/kaggle/working/ssl_dann_outputs/dann_epoch_{epoch+1}.pth\")\n\nprint(\"DANN Training hoàn tất!\")\n\n# Vẽ biểu đồ Accuracy\nplt.plot(accs)\nplt.title(\"Source Domain Accuracy per Epoch\")\nplt.xlabel(\"Epoch\")\nplt.ylabel(\"Accuracy\")\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-07T12:32:09.794595Z","iopub.execute_input":"2025-12-07T12:32:09.794872Z","iopub.status.idle":"2025-12-07T12:47:21.255876Z","shell.execute_reply.started":"2025-12-07T12:32:09.794852Z","shell.execute_reply":"2025-12-07T12:47:21.255050Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ckpt = torch.load(\"/kaggle/working/ssl_dann_outputs/dann_epoch_20.pth\", map_location=DEVICE) <-- load output \nckpt = torch.load(\"/kaggle/input/it159-final-checkpoints/ssda_outputs/dann_epoch_20.pth\", map_location=DEVICE)\n\nencoder.load_state_dict(ckpt[\"encoder\"])\nclassifier.load_state_dict(ckpt[\"classifier\"])\ndomain_disc.load_state_dict(ckpt[\"domain_disc\"])\n\nprint(\"DANN weights loaded!\")\n\nencoder.eval()\nclassifier.eval()\ndomain_disc.eval()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-07T13:53:57.871389Z","iopub.execute_input":"2025-12-07T13:53:57.872152Z","iopub.status.idle":"2025-12-07T13:53:59.929877Z","shell.execute_reply.started":"2025-12-07T13:53:57.872125Z","shell.execute_reply":"2025-12-07T13:53:59.929158Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# =========================================================\n# CELL ĐÁNH GIÁ SSDA\n# =========================================================\nimport seaborn as sns\nfrom sklearn.metrics import accuracy_score, confusion_matrix\nfrom sklearn.manifold import TSNE\nimport matplotlib.pyplot as plt\nfrom tqdm import tqdm\n\n# Đặt model ở chế độ đánh giá\nencoder.eval()\nclassifier.eval()\n\ny_true = []\ny_pred = []\n\n# --- 1. Đánh giá Accuracy và vẽ Confusion Matrix ---\nwith torch.no_grad():\n    # Dùng tgt_loader đã được định nghĩa ở trên\n    for x, y in tqdm(tgt_loader, desc=\"Evaluating on Target Domain\"):\n        x = x.to(DEVICE)\n        h = encoder(x).squeeze()\n        logits = classifier(h)\n        preds = torch.argmax(logits, dim=1)\n        \n        y_true.extend(y.cpu().numpy())\n        y_pred.extend(preds.cpu().numpy())\n\nacc = accuracy_score(y_true, y_pred)\nprint(f\"\\nTarget domain accuracy: {acc:.4f}\")\n\ncm = confusion_matrix(y_true, y_pred)\n\n# SỬA LỖI Ở ĐÂY: Dùng 'classes_list' thay vì 'classes'\nplt.figure(figsize=(6, 4))\nsns.heatmap(cm, annot=True, fmt='d', xticklabels=classes_list, yticklabels=classes_list, cmap='rocket')\nplt.title(\"Confusion Matrix on Target Domain\")\nplt.xlabel(\"Predicted Label\")\nplt.ylabel(\"True Label\")\nplt.show()\n\n\n# --- 2. Vẽ t-SNE ---\nprint(\"\\nGenerating t-SNE plot...\")\n# Lấy một batch lớn để vẽ cho đẹp\ntry:\n    sample_loader = DataLoader(tgt_dataset, batch_size=512, shuffle=True)\n    batch_x, batch_y = next(iter(sample_loader))\nexcept StopIteration:\n    print(\"Cannot get a batch for t-SNE, skipping.\")\n    batch_x, batch_y = None, None\n\nif batch_x is not None:\n    with torch.no_grad():\n        emb = encoder(batch_x.to(DEVICE)).squeeze().cpu().numpy()\n        \n    tsne = TSNE(n_components=2, random_state=42, perplexity=30, n_iter=1000)\n    emb2 = tsne.fit_transform(emb)\n    \n    # SỬA LỖI Ở ĐÂY: Dùng 'classes_list' thay vì 'classes'\n    hue_labels = [classes_list[i] for i in batch_y.numpy()]\n    \n    plt.figure(figsize=(6, 4))\n    sns.scatterplot(x=emb2[:,0], y=emb2[:,1], hue=hue_labels, legend='full')\n    plt.title(\"t-SNE of Target Embeddings\")\n    plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n    plt.grid(True)\n    plt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-07T13:54:02.426688Z","iopub.execute_input":"2025-12-07T13:54:02.427432Z","iopub.status.idle":"2025-12-07T13:54:40.822270Z","shell.execute_reply.started":"2025-12-07T13:54:02.427399Z","shell.execute_reply":"2025-12-07T13:54:40.821572Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Experiment B: SSL-FT (SimCLR + Supervised Fine-tuning)\n\nThis experiment serves as an important \"ablation study\". We remove the Domain Adaptation (DA) component from our main framework to isolate and measure the contribution of the Self-Supervised Learning (SSL) stage alone. The SimCLR-pretrained encoder is fine-tuned directly on the labeled source data, just like the baseline.\n\n- **Hypothesis:** This method will outperform the standard baseline, demonstrating the value of in-domain self-supervised pre-training. Comparing it to SSDA will reveal the actual impact of the DANN component.","metadata":{}},{"cell_type":"code","source":"# =============================================================================\n# SSL - FT (SIMCLR + SUPERVISED FINE-TUNING)\n# =============================================================================\nimport torch\nimport torch.nn as nn\nfrom torchvision import models\nfrom tqdm import tqdm\nfrom sklearn.metrics import accuracy_score, classification_report\nimport os\nimport numpy as np\nfrom sklearn.utils.class_weight import compute_class_weight\n\nprint(\"Bắt đầu Phương pháp Tối ưu (size=224)...\")\n\n# --- 1. Load \"Bộ não\" SimCLR ---\n# Khởi tạo kiến trúc ResNet50 rỗng\nfinetune_encoder = models.resnet50(weights=None) \nfeat_dim_ft = finetune_encoder.fc.in_features\nfinetune_encoder.fc = nn.Identity()\nmodules = list(finetune_encoder.children())[:-1] \nfinetune_encoder = nn.Sequential(*modules).to(DEVICE)\n\n# Load checkpoint SimCLR bạn đã có\nsimclr_ckpt_path = \"/kaggle/input/it159-final-checkpoints/simclr_outputs/simclr_epoch_50(x1)(no_freeze).pth\" # <-- KIỂM TRA LẠI ĐƯỜNG DẪN NÀY!\nif not os.path.exists(simclr_ckpt_path):\n    raise FileNotFoundError(f\"Không tìm thấy file checkpoint SimCLR tại: {simclr_ckpt_path}\")\nsimclr_ckpt = torch.load(simclr_ckpt_path, map_location=DEVICE)\nfinetune_encoder.load_state_dict(simclr_ckpt['encoder'])\nprint(\"Encoder đã được load từ checkpoint SimCLR!\")\n\nclasses_list = src_dataset.classes\n\n# Khởi tạo Classifier mới\nfinetune_classifier = nn.Linear(feat_dim_ft, len(classes_list)).to(DEVICE)\n\n# --- 2. Tính Class Weights ---\nall_labels = src_loader.dataset.targets\nclass_weights_arr = compute_class_weight('balanced', classes=np.unique(all_labels), y=all_labels)\nclass_weights = torch.tensor(class_weights_arr, dtype=torch.float).to(DEVICE)\n\n# --- 3. Định nghĩa Optimizer, Scheduler, và Loss ---\nfinetune_epochs = 20\noptimizer_ft = torch.optim.SGD(\n    list(finetune_encoder.parameters()) + list(finetune_classifier.parameters()),\n    lr=1e-3, momentum=0.9, weight_decay=5e-4\n)\ncriterion_ft = nn.CrossEntropyLoss(weight=class_weights)\nscheduler_ft = torch.optim.lr_scheduler.StepLR(optimizer_ft, step_size=10, gamma=0.1)\n\n# --- 4. Huấn luyện ---\nfor epoch in range(finetune_epochs):\n    finetune_encoder.train()\n    finetune_classifier.train()\n    pbar = tqdm(src_loader, desc=f\"Fine-tune Epoch {epoch+1}/{finetune_epochs}\")\n    for images, labels in pbar:\n        images, labels = images.to(DEVICE), labels.to(DEVICE)\n        features = finetune_encoder(images).squeeze()\n        outputs = finetune_classifier(features)\n        loss = criterion_ft(outputs, labels)\n        optimizer_ft.zero_grad()\n        loss.backward()\n        optimizer_ft.step()\n    scheduler_ft.step()\n\n# --- 5. Đánh giá ---\nfinetune_encoder.eval()\nfinetune_classifier.eval()\ny_true_final, y_pred_final = [], []\nwith torch.no_grad():\n    for images, labels in tqdm(tgt_loader, desc=\"Evaluating Final Model\"):\n        images = images.to(DEVICE)\n        features = finetune_encoder(images).squeeze()\n        outputs = finetune_classifier(features)\n        _, predicted = torch.max(outputs.data, 1)\n        y_true_final.extend(labels.cpu().numpy())\n        y_pred_final.extend(predicted.cpu().numpy())\nfinal_accuracy = accuracy_score(y_true_final, y_pred_final)\n\n# --- 6. In kết quả ---\nprint(\"\\n\" + \"=\"*60)\nprint(f\"KẾT QUẢ PHƯƠNG PHÁP TỐI ƯU (SimCLR + Supervised FT)\")\nprint(f\"   - Accuracy trên Target: {final_accuracy * 100:.2f}%\")\nprint(\"=\"*60)\nprint(\"\\nClassification Report (Final Model):\\n\")\nprint(classification_report(y_true_final, y_pred_final, target_names=classes_list, digits=4))\n\n# --- 7. LƯU LẠI CHECKPOINT CUỐI CÙNG (Thêm vào cuối cell SSL-FT) ---\nsslft_output_dir = '/kaggle/working/ssl_ft_final_outputs'\nos.makedirs(sslft_output_dir, exist_ok=True)\ntorch.save(finetune_encoder.state_dict(), os.path.join(sslft_output_dir, 'ssl_ft_encoder_last.pth'))\ntorch.save(finetune_classifier.state_dict(), os.path.join(sslft_output_dir, 'ssl_ft_classifier_last.pth'))\nprint(f\"Đã lưu checkpoint của SSL-FT vào thư mục: {sslft_output_dir}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-07T12:57:48.336920Z","iopub.execute_input":"2025-12-07T12:57:48.337537Z","iopub.status.idle":"2025-12-07T13:09:17.134120Z","shell.execute_reply.started":"2025-12-07T12:57:48.337513Z","shell.execute_reply":"2025-12-07T13:09:17.133370Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Experiment C: Supervised Baseline (ImageNet Pre-trained)\n\nThis is our strong baseline for comparison. It represents the standard and most common approach in medical image analysis: taking a ResNet-50 model pre-trained on ImageNet and directly fine-tuning it on the labeled source data. It does not use SimCLR pre-training on HAM10000 or any domain adaptation techniques.\n\n- **Hypothesis:** This robust baseline serves as the \"golden standard\" or the \"score to beat\". The performance of our proposed methods will be measured against this baseline.","metadata":{}},{"cell_type":"code","source":"# ===================================================================\n# BASELINE TỐI ƯU (IMAGE-NET SUPERVISED)\n# ===================================================================\nimport torch\nimport torch.nn as nn\nfrom torchvision import models\nfrom tqdm import tqdm\nfrom sklearn.metrics import accuracy_score, classification_report\nimport numpy as np\nfrom sklearn.utils.class_weight import compute_class_weight\n\nprint(\"Bắt đầu chạy Baseline Tối ưu (size=224)...\")\nclasses_list = src_dataset.classes\n\n# --- 1. Khởi tạo mô hình (từ ImageNet) ---\nbaseline_encoder = models.resnet50(weights=models.ResNet50_Weights.DEFAULT)\nfeat_dim_bl = baseline_encoder.fc.in_features\nbaseline_encoder.fc = nn.Identity()\nmodules_bl = list(baseline_encoder.children())[:-1]\nbaseline_encoder = nn.Sequential(*modules_bl).to(DEVICE)\nbaseline_classifier = nn.Linear(feat_dim_bl, len(classes_list)).to(DEVICE)\n\n# --- 2. Tính Class Weights ---\nall_labels = src_loader.dataset.targets\nclass_weights_arr = compute_class_weight('balanced', classes=np.unique(all_labels), y=all_labels)\nclass_weights = torch.tensor(class_weights_arr, dtype=torch.float).to(DEVICE)\n\n# --- 3. Định nghĩa Optimizer, Scheduler, và Loss ---\nbaseline_epochs = 20\noptimizer_bl = torch.optim.SGD(\n    list(baseline_encoder.parameters()) + list(baseline_classifier.parameters()),\n    lr=1e-3, momentum=0.9, weight_decay=5e-4\n)\ncriterion_bl = nn.CrossEntropyLoss(weight=class_weights)\nscheduler_bl = torch.optim.lr_scheduler.StepLR(optimizer_bl, step_size=10, gamma=0.1)\n\n# --- 4. Huấn luyện ---\nfor epoch in range(baseline_epochs):\n    baseline_encoder.train()\n    baseline_classifier.train()\n    pbar = tqdm(src_loader, desc=f\"Baseline Epoch {epoch+1}/{baseline_epochs}\")\n    for images, labels in pbar:\n        images, labels = images.to(DEVICE), labels.to(DEVICE)\n        features = baseline_encoder(images).squeeze()\n        outputs = baseline_classifier(features)\n        loss = criterion_bl(outputs, labels)\n        optimizer_bl.zero_grad()\n        loss.backward()\n        optimizer_bl.step()\n    scheduler_bl.step()\n\n# --- 5. Đánh giá ---\nbaseline_encoder.eval()\nbaseline_classifier.eval()\ny_true_bl, y_pred_bl = [], []\nwith torch.no_grad():\n    for images, labels in tqdm(tgt_loader, desc=\"Evaluating Baseline\"):\n        images = images.to(DEVICE)\n        features = baseline_encoder(images).squeeze()\n        outputs = baseline_classifier(features)\n        _, predicted = torch.max(outputs.data, 1)\n        y_true_bl.extend(labels.cpu().numpy())\n        y_pred_bl.extend(predicted.cpu().numpy())\nbaseline_accuracy = accuracy_score(y_true_bl, y_pred_bl)\n\n# --- 6. In kết quả ---\nprint(\"\\n\" + \"=\"*50)\nprint(f\"KẾT QUẢ BASELINE TỐI ƯU\")\nprint(f\"   - Accuracy trên Target: {baseline_accuracy * 100:.2f}%\")\nprint(\"=\"*50)\nprint(\"\\nClassification Report (Baseline):\\n\")\nprint(classification_report(y_true_bl, y_pred_bl, target_names=classes_list, digits=4))\n\n# --- 7. LƯU LẠI CHECKPOINT CUỐI CÙNG (Thêm vào cuối cell Baseline) ---\nbaseline_output_dir = '/kaggle/working/baseline_final_outputs'\nos.makedirs(baseline_output_dir, exist_ok=True)\ntorch.save(baseline_encoder.state_dict(), os.path.join(baseline_output_dir, 'baseline_encoder_last.pth'))\ntorch.save(baseline_classifier.state_dict(), os.path.join(baseline_output_dir, 'baseline_classifier_last.pth'))\nprint(f\"Đã lưu checkpoint của Baseline vào thư mục: {baseline_output_dir}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-07T13:14:45.723310Z","iopub.execute_input":"2025-12-07T13:14:45.723636Z","iopub.status.idle":"2025-12-07T13:26:06.018355Z","shell.execute_reply.started":"2025-12-07T13:14:45.723610Z","shell.execute_reply":"2025-12-07T13:26:06.017387Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Deep Dive: Why the Results Follow this Hierarchy?\n\nAfter rigorous experimentation, we observed the following performance hierarchy:\n**Supervised Baseline (83.33%) > SSDA (75.67%) > SSL-FT (70.73%)**\n\nHere is the technical justification for these results:\n\n### 1. The \"ImageNet Superiority\" (Baseline Advantage)\nThe **Supervised Baseline** directly fine-tunes ImageNet weights. ImageNet contains **1.2 million diverse images**, providing a massive foundational \"knowledge base\" of visual features (edges, textures, shapes). For a dataset like HAM10000 (10k images), these general-purpose features are incredibly robust and difficult to surpass through self-supervision on a much smaller scale.\n\n### 2. The \"Catastrophic Forgetting\" Phenomenon (SSDA vs. Baseline)\nDuring **Stage 1 (SimCLR)**, the model is further pre-trained on the 10,015 skin images. While this helps the model learn skin-specific patterns, it often leads to **Catastrophic Forgetting**. The model \"overwrites\" the superior general features from ImageNet with overly specific ones from a limited dataset. Because 10k images is considered a \"low-data regime\" for SSL, the trade-off resulted in a net loss of discriminative power compared to the raw ImageNet baseline.\n\n### 3. The \"DANN Uplift\" (SSDA vs. SSL-FT)\nThe most critical takeaway is the **5% accuracy increase** from SSL-FT (70.73%) to SSDA (75.67%):\n- **SSL-FT (Ablation):** Only uses SimCLR weights with simple fine-tuning. It suffers from domain shift.\n- **SSDA (Proposed):** Uses DANN to explicitly align Source and Target features.\nThis 5% uplift proves that **Domain Adaptation is technically successful**. It successfully mitigated the domain gap, even though the base features were slightly weakened by the SSL stage.\n\n### 4. Why DANN wasn't enough to beat the Baseline?\nDANN is a tool for **Domain Alignment**, not **Feature Enhancement**. It helps apply existing knowledge to a new domain, but it cannot \"create\" new knowledge. Since the foundational knowledge (Encoder weights) was slightly compromised during the SimCLR stage on a small dataset, DANN could only recover part of the performance, not exceed the high bar set by the ImageNet-Supervised baseline.\n\n---\n**Conclusion for the Reviewer:**\nOur results demonstrate that while SSDA is a powerful framework for domain shift, the **volume of pre-training data** (ImageNet vs. HAM10000) remains the dominant factor. In medical scenarios with strictly limited labels, our SSDA method provides a **significant 5% improvement** over standard SSL methods.","metadata":{}}]}